{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WhoIsWho_Coculture-TrainingAndValidation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JanRibasGarriga/WhoIsWho/blob/main/ColabNotebooks/WhoIsWho_Coculture_TrainingAndValidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xuftwsqe6CP"
      },
      "source": [
        "---\n",
        "# **WHO IS WHO - [CO-CULTURE / TRAINING & VALIDATION]**\n",
        "\n",
        "<font size = 4> WhoIsWho is Google Colab-based tool that aims to classify cells based on features related to their nuclei and their neighbours.\n",
        "\n",
        "<font size = 4> This notebook is meant to be used for the training and validation of the pipeline when using co-culture training datasets.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuKtfSonx2Jq"
      },
      "source": [
        "<font size = 4>Some blocks of the notebook are based on the paper...\n",
        "\n",
        "*   <font size = 4> Stringer, Carsen, Tim Wang, Michalis Michaelos, and Marius Pachitariu. 2021. **“Cellpose: A Generalist Algorithm for Cellular Segmentation.”** Nature Methods 18 (1): 100–106. https://doi.org/10.1038/s41592-020-01018-x\n",
        "\n",
        "  *   <font size = 4> The Original code is freely available on GitHub: https://github.com/MouseLand/cellpose\n",
        "\n",
        "<font size = 4>and the webstie...\n",
        "\n",
        "*   <font size = 4>**Transfer learning and fine-tuning**, 2017 François Chollet. https://www.tensorflow.org/tutorials/images/transfer_learning    \n",
        "\n",
        "<font size = 4>Please also cite the original sources when developing this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "DFUaV6XryI3X"
      },
      "source": [
        "#@markdown ##Double click to see the license information\n",
        "\n",
        "#--- License for \"Transfer learning and fine-tuning, 2017 François Chollet\" ---\n",
        "# MIT License\n",
        "#\n",
        "# Copyright (c) 2017 François Chollet                                                                                                                    # IGNORE_COPYRIGHT: cleared by OSS licensing\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mEvseQHYALL"
      },
      "source": [
        "# **1. Initialise the Colab session**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkalS6RcXT8C",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run this cell to check if you have GPU access.\n",
        "\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name()=='':\n",
        "  gpu_available = False\n",
        "  print(\"You are not connected to a GPU.\") \n",
        "  print(\"Please, check the selected runtime type.\") \n",
        "  print(\"Expect slow performance, especially if classification training needs to be performed.\")\n",
        "\n",
        "else:\n",
        "  gpu_available = True\n",
        "  print(\"GPU device available\")\n",
        "\n",
        "if gpu_available:\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print(\"To enable a GPU accelerator, check the selected runtime type.\")\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "\n",
        "  from psutil import virtual_memory\n",
        "  ram_gb = virtual_memory().total / 1e9\n",
        "  print(\"Your runtime has {:.1f} gigabytes of available RAM\\n\".format(ram_gb))\n",
        "  if ram_gb < 20:\n",
        "    print(\"To enable a high-RAM runtime, check the selected runtime type.\")\n",
        "  else:\n",
        "    print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_yCs1kjZL85",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run this cell to mount your Google Drive in your virtual machine of this exectuable Colab document.\n",
        "\n",
        "#@markdown * Click on the URL. \n",
        "\n",
        "#@markdown * Sign in your Google Account. \n",
        "\n",
        "#@markdown * Copy the authorization code. \n",
        "\n",
        "#@markdown * Paste the authorization code. \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSCGGPCsfKiT"
      },
      "source": [
        "# **2. Install dependencies**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6umcd2VavzvT",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Install general dependencies.\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import skimage\n",
        "from skimage.util.shape import view_as_windows\n",
        "from skimage import io\n",
        "from skimage import util\n",
        "from skimage import filters\n",
        "from skimage import img_as_ubyte, img_as_uint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import files\n",
        "\n",
        "#Colors for the warning messages\n",
        "class bcolors:\n",
        "  WARNING = '\\033[31m'\n",
        "\n",
        "W  = '\\033[0m'  # white (normal)\n",
        "R  = '\\033[31m' # red\n",
        "\n",
        "#Disable some of the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh_99dICv545",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Install Segmentation Block's dependencies.\n",
        "\n",
        "!pip install tifffile\n",
        "from tifffile import imread, imsave\n",
        "from zipfile import ZIP_DEFLATED\n",
        "\n",
        "!pip install cellpose \n",
        "from cellpose import models\n",
        "from cellpose import plot \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPxbXChB4H62",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Install Localization Block's dependencies.\n",
        "\n",
        "from skimage import morphology\n",
        "from skimage import measure\n",
        "import pandas as pd \n",
        "\n",
        "from PIL import Image\n",
        "import skimage\n",
        "from skimage import img_as_bool, img_as_ubyte\n",
        "import numpy as np\n",
        "from skimage import segmentation\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.patches as patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrKznitJ0EVZ",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Install Classification Block's dependencies.\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.data.experimental import cardinality\n",
        "from keras.layers.experimental import preprocessing\n",
        "from keras.applications import xception\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from keras.optimizers import Nadam\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "from keras.models import load_model\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm0XRn7__HvG"
      },
      "source": [
        "# **4. Data preparation**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88DTM8lfwnzg"
      },
      "source": [
        "## **4.1. Define path of the raw data**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AceeW1tUyTDk"
      },
      "source": [
        "<font size = 4> Before you run the following cell, please ensure that the training dataset has been properly stored on the respective Google Drive, following the following structure of paired images:\n",
        "\n",
        "*   Data/ \n",
        "  *   TL/\n",
        "      * image_1.tif \n",
        "      * image_2.tif \n",
        "      * image_3.tif\n",
        "      * ...\n",
        "  \n",
        "  *   DAPI/\n",
        "      * image_1.tif \n",
        "      * image_2.tif \n",
        "      * image_3.tif\n",
        "      * ...\n",
        "\n",
        "  *   EGFP/\n",
        "      * image_1.tif \n",
        "      * image_2.tif \n",
        "      * image_3.tif\n",
        "      * ...\n",
        "\n",
        "  *   mCherry/\n",
        "      * image_1.tif \n",
        "      * image_2.tif \n",
        "      * image_3.tif\n",
        "      * ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4gG1wwse5uz",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Please specify the path to where the data is located in your Google Drive.\n",
        "\n",
        "#@markdown <font size = 4> **`data_folder_path`**: is the path of the \"Data/\" folder illustrated above.\n",
        "\n",
        "data_folder_path = '' #@param {type:\"string\"}\n",
        "data_folder_path = data_folder_path + \"/\"\n",
        "\n",
        "if os.path.isdir(data_folder_path):\n",
        "  tl_folder_path = os.path.join(data_folder_path, \"TL/\")\n",
        "  dapi_folder_path = os.path.join(data_folder_path, \"DAPI/\")\n",
        "  EGFP_folder_path = os.path.join(data_folder_path, \"EGFP/\")\n",
        "  mCherry_folder_path = os.path.join(data_folder_path, \"mCherry/\")\n",
        "\n",
        "else:\n",
        "  print(R+'!! WARNING: The path selected does not exist !!'+W)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9KeyrTN-SjX",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run this cell to create the directory structure in /content. \n",
        "\n",
        "whoiswho_folder_path = \"/content/WhoIsWho\"\n",
        "\n",
        "if os.path.isdir(whoiswho_folder_path):\n",
        "  shutil.rmtree(whoiswho_folder_path)\n",
        "\n",
        "if not os.path.isdir(whoiswho_folder_path):\n",
        "\n",
        "  os.mkdir(whoiswho_folder_path)\n",
        "\n",
        "  # Defining PATCHES paths and creating folders and subfolders\n",
        "  patches_folder_path = os.path.join(whoiswho_folder_path, \"Patches\")\n",
        "  os.mkdir(patches_folder_path)\n",
        "\n",
        "  tlpatches_folder_path = os.path.join(patches_folder_path, \"TL/\")\n",
        "  os.mkdir(tlpatches_folder_path)\n",
        "  \n",
        "  dapipatches_folder_path = os.path.join(patches_folder_path, \"DAPI/\")\n",
        "  os.mkdir(dapipatches_folder_path)\n",
        "\n",
        "  cellposelabelpatches_folder_path = os.path.join(patches_folder_path, \"CellposeLabel/\")\n",
        "  os.mkdir(cellposelabelpatches_folder_path)\n",
        "\n",
        "  EGFPpatches_folder_path = os.path.join(patches_folder_path, \"EGFP/\")\n",
        "  os.mkdir(EGFPpatches_folder_path)\n",
        "  \n",
        "  mCherrypatches_folder_path = os.path.join(patches_folder_path, \"mCherry/\")\n",
        "  os.mkdir(mCherrypatches_folder_path)\n",
        " \n",
        "  regionpropsdf_folder_path = os.path.join(patches_folder_path, \"Regionpropsdf/\")\n",
        "  os.mkdir(regionpropsdf_folder_path)\n",
        "  \n",
        "  # Defining SNIPPETS paths and creating folders and subfolders\n",
        "  snippets_folder_path = os.path.join(whoiswho_folder_path, \"Snippets\")\n",
        "  os.mkdir(snippets_folder_path)\n",
        "\n",
        "  tlsnippets_folder_path = os.path.join(snippets_folder_path, \"TL/\")\n",
        "  os.mkdir(tlsnippets_folder_path)\n",
        "  \n",
        "  # Defining FIGURES paths and creating folders and subfolders\n",
        "  figures_folder_path = os.path.join(whoiswho_folder_path, \"Figures\")\n",
        "  os.mkdir(figures_folder_path)\n",
        "\n",
        "  generalfigures_folder_path = os.path.join(figures_folder_path, \"GeneralFigures\")\n",
        "  os.mkdir(generalfigures_folder_path)\n",
        "\n",
        "  cellposefigures_folder_path = os.path.join(figures_folder_path, \"CellposeFigures\")\n",
        "  os.mkdir(cellposefigures_folder_path)\n",
        "\n",
        "  # Defining MODEL paths and creating folders and subfolders\n",
        "  model_folder_path = os.path.join(whoiswho_folder_path, \"Model\")\n",
        "  os.mkdir(model_folder_path)\n",
        "\n",
        "print(\"/content/WhoIsWho directory just created\")\n",
        "print(\"Files will be stored under this directory\")\n",
        "print(\"Remember to run the last section of the notebook to select and download the results to your computer\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vegQ_Ng1hhT5",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run to display and save an example of an images.\n",
        "\n",
        "random_choice = random.choice(os.listdir(tl_folder_path))\n",
        "\n",
        "tl_sample = io.imread(os.path.join(tl_folder_path, random_choice), as_gray = True)\n",
        "dapi_sample = io.imread(os.path.join(dapi_folder_path, random_choice), as_gray = True)\n",
        "EGFP_sample = io.imread(os.path.join(EGFP_folder_path, random_choice), as_gray = True)\n",
        "mCherry_sample = io.imread(os.path.join(mCherry_folder_path, random_choice), as_gray = True)\n",
        "\n",
        "print(random_choice)\n",
        "\n",
        "f = plt.figure(figsize = (20,10))\n",
        "\n",
        "plt.subplot(1,4,1)\n",
        "plt.imshow(tl_sample, interpolation = 'nearest', cmap = 'gray')\n",
        "plt.title('TL image')\n",
        "plt.axis('off');\n",
        "\n",
        "plt.subplot(1,4,2)\n",
        "plt.imshow(dapi_sample, interpolation = 'nearest', cmap = 'gray')\n",
        "plt.title('DAPI image')\n",
        "plt.axis('off');\n",
        "\n",
        "plt.subplot(1,4,3)\n",
        "plt.imshow(EGFP_sample, interpolation = 'nearest', cmap = 'gray')\n",
        "plt.title('EGFP image')\n",
        "plt.axis('off');\n",
        "\n",
        "plt.subplot(1,4,4)\n",
        "plt.imshow(mCherry_sample, interpolation = 'nearest', cmap = 'gray')\n",
        "plt.title('mCherry image')\n",
        "plt.axis('off');\n",
        "\n",
        "base_name_image = random_choice.replace(\".tif\", \"\")\n",
        "figure_name = (f\"Image_{base_name_image}\")\n",
        "\n",
        "plt.savefig(os.path.join(generalfigures_folder_path, figure_name + \".png\"),\n",
        "            dpi = 300, format = \"png\", bbox_inches = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS3zDT1W_dRE"
      },
      "source": [
        "## **4.2. Creation of patches**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLRq6MGH7bXr",
        "cellView": "form"
      },
      "source": [
        "def fittingPowerOfTwo(number):\n",
        "  n = 0\n",
        "  while 2**n <= number:\n",
        "    n += 1 \n",
        "  return 2**(n-1)\n",
        "\n",
        "def estimatePatchSize(data_path, max_width = 512, max_height = 512):\n",
        "\n",
        "  files = os.listdir(data_path)\n",
        "  \n",
        "  # Get the size of the first image found in the folder and initialise the variables to that\n",
        "  n = 0 \n",
        "  while os.path.isdir(os.path.join(data_path, files[n])):\n",
        "    n += 1\n",
        "  (height_min, width_min) = Image.open(os.path.join(data_path, files[n])).size\n",
        "\n",
        "  # Screen the size of all dataset to find the minimum image size\n",
        "  for file_ in files:\n",
        "    if not os.path.isdir(os.path.join(data_path, file_)):\n",
        "      (height, width) = Image.open(os.path.join(data_path, file_)).size\n",
        "      if width < width_min:\n",
        "        width_min = width\n",
        "      if height < height_min:\n",
        "        height_min = height\n",
        "    break\n",
        "  \n",
        "  # Find the power of patches that will fit within the smallest dataset\n",
        "  width_min, height_min = (fittingPowerOfTwo(width_min), fittingPowerOfTwo(height_min))\n",
        "\n",
        "  # Clip values at maximum permissible values\n",
        "  if width_min > max_width:\n",
        "    width_min = max_width\n",
        "\n",
        "  if height_min > max_height:\n",
        "    height_min = max_height\n",
        "  \n",
        "  return (width_min, height_min)\n",
        "  \n",
        "#@markdown ##Run to define the dimensions of the patches. \n",
        "\n",
        "#@markdown <font size = 4> The notebook crops the data in patches of fixed size. The largest 2^n x 2^n patch that fits in the data being used so as to ensure no overlapping between patches from the same image. Yet, the largest patch available is 512 x 512 in order to preserve the stability of the entire pipeline.\n",
        "\n",
        "patch_width, patch_height = estimatePatchSize(\n",
        "    tl_folder_path\n",
        ")\n",
        "\n",
        "print(f\"Patch dimensions (patch_width, patch_height): ({patch_width}, {patch_height}).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkMToTsQ3ANj",
        "cellView": "form"
      },
      "source": [
        "#@markdown ## Run to crop and filter patches of each image.\n",
        "\n",
        "#@markdown ### Please select whether you want to use the default patches parameters.\n",
        "\n",
        "Use_default_patches_parameters = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### Otherwise, please define the parameter:\n",
        "\n",
        "#@markdown <font size = 4>**`ratio_cells_vs_background`**: threshold of the fraction (%) of foreground pixels against the fraction of background pixels for each patch so as to be considered for the following steps of the pipeline. When `Use_default_patches_parameters` is enabled, the parameter is set to 5%.\n",
        "\n",
        "ratio_cells_vs_background = 5 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "\n",
        "if Use_default_patches_parameters:\n",
        "  ratio_cells_vs_background = 5\n",
        "\n",
        "def convert2mask(image, threshold = None):\n",
        "  if threshold == None:\n",
        "    threshold = filters.threshold_otsu(image)\n",
        "  image[image > threshold] = 255\n",
        "  image[image <= threshold] = 0\n",
        "  return image\n",
        "\n",
        "def normalizeMinMax(x, dtype = np.float32):\n",
        "  x = x.astype(dtype,copy=False)\n",
        "  x = (x - np.amin(x)) / (np.amax(x) - np.amin(x))\n",
        "  return x\n",
        "\n",
        "def create_patches(tl_folder_path, dapi_folder_path, EGFP_folder_path, mCherry_folder_path, patch_width, patch_height):\n",
        "  patch_num = 1\n",
        "\n",
        "  for filename in tqdm(sorted(os.listdir(tl_folder_path))):\n",
        "\n",
        "    basename_filename = os.path.splitext(filename)[0]\n",
        "\n",
        "    tl = io.imread(os.path.join(tl_folder_path, filename))\n",
        "    dapi = io.imread(os.path.join(dapi_folder_path, filename))\n",
        "    mask = img_as_ubyte(convert2mask(io.imread(os.path.join(dapi_folder_path, filename))))\n",
        "    EGFP = io.imread(os.path.join(EGFP_folder_path, filename))\n",
        "    mCherry = io.imread(os.path.join(mCherry_folder_path, filename))\n",
        "\n",
        "    # Using view_as_windows with step size equal to the patch size to ensure there is no overlap\n",
        "    patches_tl = view_as_windows(tl, (patch_width, patch_height), (patch_width, patch_height))\n",
        "    patches_dapi = view_as_windows(dapi, (patch_width, patch_height), (patch_width, patch_height))\n",
        "    patches_mask = view_as_windows(mask, (patch_width, patch_height), (patch_width, patch_height))\n",
        "    patches_EGFP = view_as_windows(EGFP, (patch_width, patch_height), (patch_width, patch_height))\n",
        "    patches_mCherry = view_as_windows(mCherry, (patch_width, patch_height), (patch_width, patch_height))\n",
        "\n",
        "    patches_tl = patches_tl.reshape(patches_tl.shape[0]*patches_tl.shape[1], patch_width, patch_height)\n",
        "    patches_dapi = patches_dapi.reshape(patches_dapi.shape[0]*patches_dapi.shape[1], patch_width, patch_height)\n",
        "    patches_mask = patches_mask.reshape(patches_mask.shape[0]*patches_mask.shape[1], patch_width, patch_height)\n",
        "    patches_EGFP = patches_EGFP.reshape(patches_EGFP.shape[0]*patches_EGFP.shape[1], patch_width, patch_height)\n",
        "    patches_mCherry = patches_mCherry.reshape(patches_mCherry.shape[0]*patches_mCherry.shape[1], patch_width, patch_height)\n",
        "\n",
        "    for i in range(patches_tl.shape[0]):\n",
        "      tl_save_path = os.path.join(tlpatches_folder_path, basename_filename+'--PATCH'+str(patch_num)+'.tif')\n",
        "      dapi_save_path = os.path.join(dapipatches_folder_path, basename_filename+'--PATCH'+str(patch_num)+'.tif')\n",
        "      EGFP_save_path = os.path.join(EGFPpatches_folder_path, basename_filename+'--PATCH'+str(patch_num)+'.tif')\n",
        "      mCherry_save_path = os.path.join(mCherrypatches_folder_path, basename_filename+'--PATCH'+str(patch_num)+'.tif')\n",
        "\n",
        "      patch_num += 1\n",
        "\n",
        "      counts = np.count_nonzero(patches_mask[i] == 255)\n",
        "      ratio = (counts / (patches_mask[i].shape[0] * patches_mask[i].shape[1])) * 100\n",
        "\n",
        "      if ratio >= ratio_cells_vs_background:\n",
        "        io.imsave(tl_save_path, img_as_ubyte(normalizeMinMax(patches_tl[i])))\n",
        "        io.imsave(dapi_save_path, img_as_ubyte(normalizeMinMax(patches_dapi[i])))\n",
        "        io.imsave(EGFP_save_path, img_as_ubyte(patches_EGFP[i]))\n",
        "        io.imsave(mCherry_save_path, img_as_ubyte(patches_mCherry[i]))\n",
        "\n",
        "create_patches(tl_folder_path, \n",
        "               dapi_folder_path,\n",
        "               EGFP_folder_path, \n",
        "               mCherry_folder_path, \n",
        "               patch_width, patch_height,\n",
        ")\n",
        "\n",
        "print(\"Patches just created\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmlTJ2Wfmvzi",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run to display and save an example of a patch.\n",
        "\n",
        "random_choice = random.choice(os.listdir(tlpatches_folder_path))\n",
        "\n",
        "tl_sample = io.imread(os.path.join(tlpatches_folder_path, random_choice), as_gray = True)\n",
        "dapi_sample = io.imread(os.path.join(dapipatches_folder_path, random_choice), as_gray = True)\n",
        "EGFP_sample = io.imread(os.path.join(EGFPpatches_folder_path, random_choice), as_gray = True)\n",
        "mCherry_sample = io.imread(os.path.join(mCherrypatches_folder_path, random_choice), as_gray = True)\n",
        "\n",
        "print(random_choice)\n",
        "\n",
        "f = plt.figure(figsize = (20,10))\n",
        "\n",
        "plt.subplot(1,4,1)\n",
        "plt.imshow(tl_sample, interpolation = 'nearest', cmap = 'gray')\n",
        "plt.title('TL image')\n",
        "plt.axis('off');\n",
        "\n",
        "plt.subplot(1,4,2)\n",
        "plt.imshow(dapi_sample, interpolation = 'nearest', cmap = 'gray')\n",
        "plt.title('DAPI image')\n",
        "plt.axis('off');\n",
        "\n",
        "plt.subplot(1,4,3)\n",
        "plt.imshow(EGFP_sample, interpolation = 'nearest', cmap = 'gray')\n",
        "plt.title('EGFP image')\n",
        "plt.axis('off');\n",
        "\n",
        "plt.subplot(1,4,4)\n",
        "plt.imshow(mCherry_sample, interpolation = 'nearest', cmap = 'gray')\n",
        "plt.title('mCherry image')\n",
        "plt.axis('off');\n",
        "\n",
        "base_name_patch = random_choice.replace(\".tif\", \"\")\n",
        "figure_name = (f\"Patch_{base_name_patch}\")\n",
        "\n",
        "plt.savefig(os.path.join(generalfigures_folder_path, figure_name + \".png\"),\n",
        "            dpi = 300, format = \"png\", bbox_inches = None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E29LWfWpjkZU"
      },
      "source": [
        "# **5. Segmentation Block**\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIfiw3j12mog",
        "cellView": "form"
      },
      "source": [
        "#@markdown ## Run to perform image segmentation using Cellpose\n",
        "\n",
        "#@markdown ### Please select whether you want to use the default segmentation parameters.\n",
        "\n",
        "Use_default_segmentation_parameters = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### Otherwise, please define the parameters:\n",
        "\n",
        "#@markdown <font size = 4>**`object_diameter`**: approximate value of the diameter of the nuclei. If `Use_default_segmentation_parameters` is enabled, the parameter is set to 25.\n",
        "\n",
        "#@markdown <font size = 4>**`Flow_threshold`**: flow error threshold that dictates which ROIs are kept or removed. If `Use_default_segmentation_parameters` is enabled, the parameter is set to 0.4.\n",
        "\n",
        "Object_diameter = 24.5 #@param {type:\"slider\", min:7, max:30, step:1}\n",
        "\n",
        "Flow_threshold = 0.4 #@param {type:\"slider\", min:0.1, max:1.1, step:0.1}\n",
        "\n",
        "#@markdown ### Please, specify if you want to save the Cellpose's segmentation plot.\n",
        "\n",
        "#@markdown <font size = 4>**`save_cellpose`**: if enabled, Cellpose's segmentation plot will be displayed and stored.\n",
        "\n",
        "save_cellpose = False #@param {type:\"boolean\"}\n",
        "\n",
        "Channel_to_segment = [0,0]\n",
        "if Use_default_segmentation_parameters:\n",
        "  print(\"Default segmentation parameters enabled\")\n",
        "  Object_diameter = 25\n",
        "  Flow_threshold = 0.4\n",
        "\n",
        "model = models.Cellpose(gpu = True, \n",
        "                        model_type = \"nuclei\",\n",
        "                        net_avg = True,\n",
        "                        device = None,\n",
        "                        torch = False\n",
        ")\n",
        "\n",
        "for filename in sorted(os.listdir(dapipatches_folder_path)):\n",
        "\n",
        "  print(f\"Cellpose is computing labels from {filename}\")\n",
        "\n",
        "  image = io.imread(os.path.join(dapipatches_folder_path, filename))\n",
        "  masks, flows, styles, diams = model.eval(image, \n",
        "                                          batch_size = 8,\n",
        "                                          channels = Channel_to_segment,\n",
        "                                          invert = False,\n",
        "                                          normalize = False,\n",
        "                                          diameter = Object_diameter,\n",
        "                                          net_avg = True,\n",
        "                                          augment = False,\n",
        "                                          tile = True,\n",
        "                                          resample = False,\n",
        "                                          interp = True,\n",
        "                                          flow_threshold = Flow_threshold,\n",
        "                                          cellprob_threshold = 0\n",
        "  )\n",
        "  \n",
        "  if save_cellpose:\n",
        "    basename_filename = os.path.splitext(filename)[0]\n",
        "    flowi = flows[0]\n",
        "    fig = plt.figure(figsize=(12,5))\n",
        "    plot.show_segmentation(fig, image, masks, flowi, channels=[0,0])\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(cellposefigures_folder_path, str(basename_filename) + \".png\"), \n",
        "                dpi = 100, format = \"png\", bbox_inches = None)\n",
        "\n",
        "  os.chdir(cellposelabelpatches_folder_path)\n",
        "  imsave(str(filename), masks, compress = ZIP_DEFLATED)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3KdoSFP4CYU"
      },
      "source": [
        "# **6. Localization Block**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is8bIY0Fns5R",
        "cellView": "form"
      },
      "source": [
        "def measure_regionprops(labelpatch_folder_path, filename):\n",
        "  '''\n",
        "  This function computes image properties from a labelled image regions\n",
        "  The information extracted will be used to crop each cell as snippet and to measure the mean intensity on the fluorescence channels\n",
        "\n",
        "  INPUT:\n",
        "  labelatch_folder_path: folder path of the label patches\n",
        "  filename: unique name of the patch\n",
        "\n",
        "  OUTPUT:\n",
        "  regionprops_df: two-dimensional, size-mutable, potentially heterogeneous tabular data\n",
        "  '''\n",
        "  # Read label patches\n",
        "  label = io.imread(os.path.join(labelpatch_folder_path, filename))\n",
        "\n",
        "  # Compute image properties and return them as a pandas-compatible table\n",
        "  regionprops_dict = measure.regionprops_table(label, \n",
        "                                               properties = [\"label\", \n",
        "                                                             \"centroid\", \n",
        "                                                             \"area\", \n",
        "                                                             \"equivalent_diameter\"]\n",
        "  )\n",
        "\n",
        "  # Convert the image properties table to a dataframe\n",
        "  regionprops_df = pd.DataFrame()\n",
        "  regionprops_df = pd.DataFrame(regionprops_dict)\n",
        "\n",
        "  # Rename a few columns to be aligned to the structure (axis) of the data\n",
        "  regionprops_df.rename(columns={\"label\": \"cell_label\",\n",
        "                                 \"centroid-0\": \"cx\", \n",
        "                                 \"centroid-1\": \"cy\", \n",
        "                                 \"equivalent_diameter\": \"diameter\"}, \n",
        "                        inplace = True\n",
        "  )\n",
        "\n",
        "  # Remove rows in which nuclei's areas are under an specific value\n",
        "  regionprops_df = regionprops_df[regionprops_df[\"area\"] > 250]\n",
        "\n",
        "  # Calculate bounding box (bbox) coordinates and store them directly into the dataframe\n",
        "  regionprops_df[\"xmin\"] = regionprops_df[\"cx\"] - regionprops_df[\"diameter\"]\n",
        "  regionprops_df[\"xmax\"] = regionprops_df[\"cx\"] + regionprops_df[\"diameter\"]\n",
        "  regionprops_df[\"ymin\"] = regionprops_df[\"cy\"] - regionprops_df[\"diameter\"]\n",
        "  regionprops_df[\"ymax\"] = regionprops_df[\"cy\"] + regionprops_df[\"diameter\"]\n",
        "\n",
        "  # Remove rows in which bboxes' dimenions are out of patch's original dimensions\n",
        "  regionprops_df = regionprops_df[(regionprops_df[\"xmin\"] >= 0) &  \n",
        "                                  (regionprops_df[\"xmax\"] <= label.shape[1]) & \n",
        "                                  (regionprops_df[\"ymin\"] >= 0) & \n",
        "                                  (regionprops_df[\"ymax\"] <= label.shape[0])\n",
        "  ]\n",
        "\n",
        "  # Reset indexes of the dataframe and drop the previous index column\n",
        "  regionprops_df.reset_index(inplace = True)\n",
        "  regionprops_df.drop(labels = [\"index\"], axis = 1, inplace = True)\n",
        "\n",
        "  # Add the name of the snippet in the dataframe as id value\n",
        "  for idx in regionprops_df.index: \n",
        "    regionprops_df.loc[[idx], [\"snippet_id\"]] = (filename[:-4] + \"--SNIPPET\" + str(regionprops_df[\"cell_label\"][idx]) + \".png\")\n",
        "\n",
        "  return regionprops_df\n",
        "\n",
        "def compute_meanintensity(filename, labelpatch_folder_path, EGFPpatches_folder_path, mCherrypatches_folder_path, regionprops_df):\n",
        "  '''\n",
        "  This function computes the mean pixel intensity of both fluorescence channels of the nucleus in each bounding box\n",
        "  The information extracted is added to the respective row of the regionprops_df\n",
        "\n",
        "  INPUT:\n",
        "  filename: unique name of the patch\n",
        "  labelpatch_folder_path: folder path of the label patches\n",
        "  EGFPpatches_folder_path: folder path of the EGFP patchs\n",
        "  mCherrypatches_folder_path: folder path of the mCherry patches\n",
        "  regionprops_df: two-dimensional, size-mutable, potentially heterogeneous tabular data of the mask properties\n",
        "  '''\n",
        "  # Read patches as PIL images\n",
        "  EGFP = Image.open(os.path.join(EGFPpatches_folder_path, filename))\n",
        "  mCherry = Image.open(os.path.join(mCherrypatches_folder_path, filename))\n",
        "\n",
        "  # Read label patches\n",
        "  label = util.img_as_ubyte(io.imread(os.path.join(labelpatch_folder_path, filename)))\n",
        "\n",
        "  for idx in regionprops_df.index:\n",
        "    # --- MASK ---\n",
        "    # Remove neighbour cells which are on the snippet\n",
        "    region_of_interest = np.ones((label.shape[0], label.shape[1]), dtype = \"uint8\") * regionprops_df[\"cell_label\"][idx]\n",
        "    mask = (label == region_of_interest)\n",
        "\n",
        "    # Convert boolean mask patch to PIL image\n",
        "    mask = Image.fromarray(mask, mode=None)\n",
        "\n",
        "    # Crop snippet from mask patch \n",
        "    mask_snippet = mask.crop((regionprops_df[\"ymin\"][idx],\n",
        "                              regionprops_df[\"xmin\"][idx],\n",
        "                              regionprops_df[\"ymax\"][idx],\n",
        "                              regionprops_df[\"xmax\"][idx])\n",
        "    )\n",
        "    # Convert to bool numpy array \n",
        "    mask_snippet = img_as_bool(mask_snippet)\n",
        "\n",
        "    # --- EGFP ---\n",
        "    # Crop snippet from EGFP patch \n",
        "    EGFP_snippet = EGFP.crop((regionprops_df[\"ymin\"][idx],\n",
        "                              regionprops_df[\"xmin\"][idx],\n",
        "                              regionprops_df[\"ymax\"][idx],\n",
        "                              regionprops_df[\"xmax\"][idx])\n",
        "    )\n",
        "    # Convert to uint8 numpy array\n",
        "    EGFP_snippet = img_as_ubyte(np.array(EGFP_snippet))\n",
        "\n",
        "    # --- mCherry ---\n",
        "    # Crop snippet from mCherry patch \n",
        "    mCherry_snippet = mCherry.crop((regionprops_df[\"ymin\"][idx],\n",
        "                              regionprops_df[\"xmin\"][idx],\n",
        "                              regionprops_df[\"ymax\"][idx],\n",
        "                              regionprops_df[\"xmax\"][idx])\n",
        "    )\n",
        "    # Convert to uint8 numpy array\n",
        "    mCherry_snippet = img_as_ubyte(np.array(mCherry_snippet))\n",
        "\n",
        "    # --- MULTIPLICATION AND STORAGE ---\n",
        "    # Multiply mask and both fluorescence snippets\n",
        "    EGFPbymask_snippet = mask_snippet * EGFP_snippet\n",
        "    mCherrybymask_snippet = mask_snippet * mCherry_snippet\n",
        "    # Store the mean of both multiplication results into the dataframe\n",
        "    regionprops_df.loc[[idx], [\"meanintensityEGFP\"]] = EGFPbymask_snippet.mean()\n",
        "    regionprops_df.loc[[idx], [\"meanintensitymCherry\"]] = mCherrybymask_snippet.mean()\n",
        "\n",
        "def save_regionpropsdataframe(save_regionprops_folder_path, regionprops_df, filename):\n",
        "  '''\n",
        "  This function saves the dataframe of the patch being analyzed as .csv\n",
        "\n",
        "  INPUT:\n",
        "  save_folder_path: folder path in which dataframe will be saved\n",
        "  regionprops_df: two-dimensional, size-mutable, potentially heterogeneous tabular data of the mask properties\n",
        "  filename: unique name of the patch\n",
        "  '''\n",
        "  # Get the basename of the file\n",
        "  basename = filename.rstrip(\".tif\")\n",
        "\n",
        "  # Get the complete path (folder path and filename) where the dataframe will be saved\n",
        "  complete_save_path = os.path.join(save_regionprops_folder_path, \n",
        "                                    (basename + \".csv\")\n",
        "  )\n",
        "\n",
        "  # Save the dataframe as .xlsx \n",
        "  regionprops_df.to_csv(complete_save_path, index = False)\n",
        "\n",
        "def crop_tlsnippet(tlpatches_folder_path, regionprops_df, save_tlsnippet_folder_path, filename):\n",
        "  '''\n",
        "  This function saves all the snippets from the transmitted-light patch according to the bounding boxes dimenions\n",
        "\n",
        "  INPUT:\n",
        "  tlpatches_folder_path: folder path of the transmitted-light patches\n",
        "  regionprops_df: two-dimensional, size-mutable, potentially heterogeneous tabular data of the mask properties\n",
        "  save_tlsnippet_folder_path: folder path in which transmitted-light snippets will be saved\n",
        "  filename: unique name of the patch\n",
        "  '''\n",
        "  # Read patch as PIL images\n",
        "  tl_image = Image.open(os.path.join(tlpatches_folder_path, filename))\n",
        "\n",
        "  for idx in regionprops_df.index:\n",
        "    # Get the complete path (folder path and filename) where the snippet will be saved\n",
        "    complete_save_path = os.path.join(save_tlsnippet_folder_path, \n",
        "                                      (str(regionprops_df[\"snippet_id\"][idx]))\n",
        "    )\n",
        "\n",
        "    # Crop snippet from transmitted light patch \n",
        "    tl_snippet = tl_image.crop((regionprops_df[\"ymin\"][idx], \n",
        "                                regionprops_df[\"xmin\"][idx], \n",
        "                                regionprops_df[\"ymax\"][idx], \n",
        "                                regionprops_df[\"xmax\"][idx])\n",
        "    )\n",
        "\n",
        "    # Convert to uint8 numpy array\n",
        "    tl_snippet = img_as_ubyte(np.array(tl_snippet))\n",
        "\n",
        "    # Save the snippet into the folder\n",
        "    io.imsave(complete_save_path, tl_snippet)\n",
        "\n",
        "#@markdown ##Run to compute TL snippets and regionprops dataframe.\n",
        "\n",
        "print(\"Snippets being created...\")\n",
        "\n",
        "for filename in sorted(os.listdir(cellposelabelpatches_folder_path)):\n",
        "\n",
        "  check_label = io.imread(os.path.join(cellposelabelpatches_folder_path, filename))\n",
        "  if len(np.unique(check_label)) > 1:\n",
        "    regionprops_df = measure_regionprops(cellposelabelpatches_folder_path, filename)\n",
        "    compute_meanintensity(filename, cellposelabelpatches_folder_path, EGFPpatches_folder_path, mCherrypatches_folder_path, regionprops_df)\n",
        "    save_regionpropsdataframe(regionpropsdf_folder_path, regionprops_df, filename)\n",
        "    crop_tlsnippet(tlpatches_folder_path, regionprops_df, tlsnippets_folder_path, filename)\n",
        "\n",
        "  else:\n",
        "    os.remove(os.path.join(tlpatches_folder_path, filename))\n",
        "    os.remove(os.path.join(dapipatches_folder_path, filename))\n",
        "    os.remove(os.path.join(cellposelabelpatches_folder_path, filename))\n",
        "    if save_cellpose:\n",
        "      figure_filename = filename.replace(\".tif\", \".png\")\n",
        "      os.remove(os.path.join(cellposefigures_folder_path, figure_filename))\n",
        "\n",
        "print(\"\\n\") \n",
        "print(\"Snippets just created\")\n",
        "\n",
        "print(\"\\n\") \n",
        "number_of_snippets = len([filename for filename in os.listdir(tlsnippets_folder_path)])\n",
        "print(f\"TOTAL: {(number_of_snippets)} snippets\")\n",
        "\n",
        "# The tabular data computed for each patch is stored to a common tabular data.\n",
        "general_regionprops_df = pd.DataFrame(data = None, index = None, columns = None)\n",
        "for filename in sorted(os.listdir(regionpropsdf_folder_path)):\n",
        "  new_patch_df = pd.read_csv(os.path.join(regionpropsdf_folder_path, filename), index_col = False)\n",
        "  general_regionprops_df = general_regionprops_df.append(new_patch_df, ignore_index = True)\n",
        "\n",
        "# The outlier instances of the tabular data are removed\n",
        "q_low_EGFP = general_regionprops_df[\"meanintensityEGFP\"].quantile(0.01)\n",
        "q_hi_EGFP  = general_regionprops_df[\"meanintensityEGFP\"].quantile(0.99)\n",
        "q_low_mCherry = general_regionprops_df[\"meanintensitymCherry\"].quantile(0.01)\n",
        "q_hi_mCherry  = general_regionprops_df[\"meanintensitymCherry\"].quantile(0.99)\n",
        "\n",
        "filtered_general_regionprops_df = general_regionprops_df[\n",
        "                                                  (general_regionprops_df[\"meanintensityEGFP\"] < q_hi_EGFP) & \n",
        "                                                  (general_regionprops_df[\"meanintensityEGFP\"] > q_low_EGFP) & \n",
        "                                                  (general_regionprops_df[\"meanintensitymCherry\"] < q_hi_mCherry) & \n",
        "                                                  (general_regionprops_df[\"meanintensitymCherry\"] > q_low_mCherry)\n",
        "]\n",
        "\n",
        "# Compute the log value of each meanintensityEGFP and meanintensityEGFP attributes.\n",
        "filtered_general_regionprops_df['meanintensityEGFP_log'] = np.log2(filtered_general_regionprops_df['meanintensityEGFP'])\n",
        "filtered_general_regionprops_df['meanintensitymCherry_log'] = np.log2(filtered_general_regionprops_df['meanintensitymCherry'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZjD-itTPUVt"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQF8xSG6Ex2u"
      },
      "source": [
        "<font size = 4> The two following steps of the Localization Block are meant to be used to manually cluster each group of cells according to the cell line they belong to. Thus, you should repetitively run both cells one after the other until each population of cells has been labelled.\n",
        "\n",
        "<font size = 4> On the first cell of code, you should use the sliders displayed in order to adjust the position and shape of the rectangle to the proper location. Then, on the following cell of code, you shall specify the name of the cell line. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRiC0H8GPr-I",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run to plot the distribution of the TL snippets according to the mean intensity of both fluorescence channels.\n",
        "\n",
        "xmin = filtered_general_regionprops_df[\"meanintensityEGFP_log\"].min() - 0.25\n",
        "xmax = filtered_general_regionprops_df[\"meanintensityEGFP_log\"].max() + 0.25\n",
        "xmean = filtered_general_regionprops_df[\"meanintensityEGFP_log\"].mean()\n",
        "ymin = filtered_general_regionprops_df[\"meanintensitymCherry_log\"].min() - 0.25 \n",
        "ymax = filtered_general_regionprops_df[\"meanintensitymCherry_log\"].max() + 0.25\n",
        "ymean = filtered_general_regionprops_df[\"meanintensitymCherry_log\"].mean()\n",
        "\n",
        "filtered_general_regionprops_df[\"cell_line\"] = \"undefined\"\n",
        "cell_lines_list = []\n",
        "\n",
        "def update_plot(x, y, width, height):\n",
        "  fig, ax = plt.subplots(figsize=(16,6))\n",
        "  ax.scatter(\n",
        "    x = filtered_general_regionprops_df[\"meanintensityEGFP_log\"],\n",
        "    y = filtered_general_regionprops_df[\"meanintensitymCherry_log\"],\n",
        "    alpha = 0.2\n",
        "  )\n",
        "\n",
        "  plt.xlim(xmin, xmax)\n",
        "  plt.ylim(ymin, ymax)\n",
        "  plt.xlabel(\"Mean nuclear pixel intensity EGFP (AU)\", fontsize = 12) \n",
        "  plt.ylabel(\"Mean nuclear pixel intensity mCherry (AU)\", fontsize = 12) \n",
        "  ax.add_patch(\n",
        "      patches.Rectangle((x,y), width, height, angle = 0, color ='forestgreen', alpha = 0.5\n",
        "  ))\n",
        "\n",
        "x_slider = widgets.FloatSlider(min = xmin, max = xmax, step = 0.1, value = xmean)\n",
        "y_slider = widgets.FloatSlider(min = ymin, max = ymax, step = 0.1, value = ymean)\n",
        "width_slider = widgets.FloatSlider(min = 0.25, max = 2, step = 0.1, value = 1)\n",
        "height_slider = widgets.FloatSlider(min = 0.25, max = 2, step = 0.1, value = 1)\n",
        "\n",
        "interactive_plot = widgets.interactive(\n",
        "    update_plot, \n",
        "    x = x_slider,\n",
        "    y = y_slider,\n",
        "    width = width_slider,\n",
        "    height = height_slider\n",
        ")\n",
        "output = interactive_plot.children[-1]\n",
        "display(interactive_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrqOwa0nNNS8",
        "cellView": "form"
      },
      "source": [
        "#@markdown ## Run to label the cells just selected.\n",
        "\n",
        "#@markdown <font size = 4>**`cell_line_name`**: name of the cell line selected by the green rectangle in the previous step.\n",
        "\n",
        "cell_line_name = \"\" #@param {type:\"string\"}\n",
        "cell_lines_list.append(cell_line_name)\n",
        "\n",
        "for idx in filtered_general_regionprops_df.index:\n",
        "  if (((filtered_general_regionprops_df[\"meanintensityEGFP_log\"][idx]) >= x_slider.value) & ((filtered_general_regionprops_df[\"meanintensityEGFP_log\"][idx]) <= (x_slider.value + width_slider.value)) & ((filtered_general_regionprops_df[\"meanintensitymCherry_log\"][idx]) >= (y_slider.value)) & ((filtered_general_regionprops_df[\"meanintensitymCherry_log\"][idx]) <= (y_slider.value + height_slider.value))):\n",
        "    filtered_general_regionprops_df[\"cell_line\"][idx] = cell_line_name\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "ax = sns.scatterplot(\n",
        "    x = filtered_general_regionprops_df[\"meanintensityEGFP_log\"],\n",
        "    y = filtered_general_regionprops_df[\"meanintensitymCherry_log\"],\n",
        "    alpha = 0.2,\n",
        "    hue = filtered_general_regionprops_df[\"cell_line\"]\n",
        ")\n",
        "ax.set_xlabel(\"Mean nuclear pixel intensity EGFP (AU)\", fontsize = 12) \n",
        "ax.set_ylabel(\"Mean nuclear pixel intensity mCherry (AU)\", fontsize = 12)\n",
        "\n",
        "plt.savefig(os.path.join(generalfigures_folder_path, \"CocultureScatterPlot\" + \".png\"),\n",
        "                dpi = 300, format = \"png\", bbox_inches = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbPQ453yPXyQ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlpbnfYqNNQL",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run to organize the snippets into the respective folders.\n",
        "\n",
        "number_of_snippets = []\n",
        "\n",
        "for cell_line in sorted(cell_lines_list):\n",
        "  tlsnippets_cellline_folder_path = os.path.join(tlsnippets_folder_path, cell_line)\n",
        "  os.mkdir(tlsnippets_cellline_folder_path)\n",
        "  for idx in filtered_general_regionprops_df.index:\n",
        "    if filtered_general_regionprops_df[\"cell_line\"][idx] == cell_line:\n",
        "      shutil.move(os.path.join(tlsnippets_folder_path, filtered_general_regionprops_df[\"snippet_id\"][idx]),\n",
        "                  os.path.join(tlsnippets_cellline_folder_path, filtered_general_regionprops_df[\"snippet_id\"][idx]))\n",
        "  number_of_files = len([filename for filename in os.listdir(tlsnippets_cellline_folder_path) if os.path.isfile(os.path.join(tlsnippets_cellline_folder_path, filename))])\n",
        "  number_of_snippets.append(len([filename for filename in os.listdir(tlsnippets_cellline_folder_path) if os.path.isfile(os.path.join(tlsnippets_cellline_folder_path, filename))]))\n",
        "  print(f\"{cell_line} snippets have just been stored in the respective folder.\")\n",
        "  print(f\"{number_of_files} {cell_line} snippets.\")\n",
        "print(\"\\n\") \n",
        "print(f\"TOTAL: {sum(number_of_snippets)} snippets of {len(cell_lines_list)} cell lines.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1LeaBwH9tN4",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run to remove the \"undefined\" snippets.\n",
        "\n",
        "filelist = [f for f in os.listdir(tlsnippets_folder_path) if f.endswith(\".png\") ]\n",
        "for filename in filelist:\n",
        "  os.remove(os.path.join(tlsnippets_folder_path, filename))\n",
        "\n",
        "complete_save_path = os.path.join(regionpropsdf_folder_path, \"_filtered_general_regionprops.csv\")\n",
        "filtered_general_regionprops_df.to_csv(complete_save_path, index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7ZZWwe7ndYg",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run to display and save an example of each snippet.\n",
        "\n",
        "i = 0\n",
        "\n",
        "for cell_line_name in sorted(cell_lines_list):\n",
        "  tlsnippets_cellline_folder_path = os.path.join(tlsnippets_folder_path, cell_line_name)\n",
        "\n",
        "  for filename in os.listdir(tlsnippets_cellline_folder_path):\n",
        "\n",
        "    tl_sample = io.imread(os.path.join(tlsnippets_cellline_folder_path, filename), as_gray = True)\n",
        "\n",
        "    print(filename)\n",
        "\n",
        "    f = plt.figure(figsize = (5,5))\n",
        "\n",
        "    plt.subplot(1,1,1)\n",
        "    plt.imshow(tl_sample, interpolation = 'nearest', cmap = 'gray')\n",
        "    plt.title(f'{cell_line_name} Bright-field snippet ')\n",
        "    plt.axis('off');\n",
        "\n",
        "    base_name_snippet = filename.replace(\".png\", \"\")\n",
        "    figure_name = (f\"Snippet{cell_line_name}_{base_name_patch}\")\n",
        "\n",
        "    plt.savefig(os.path.join(generalfigures_folder_path, figure_name + \".png\"),\n",
        "                dpi = 300, format = \"png\", bbox_inches = None)\n",
        "    \n",
        "    break\n",
        "\n",
        "  i += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa1NpOPd6wuL"
      },
      "source": [
        "# **7. Classification Block**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7rDKgx7PEGK"
      },
      "source": [
        "## **7.1. Define main parameters**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQfyQ2P-xNAf",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Please specify the path where the model is going to be saved.\n",
        "\n",
        "#@markdown <font size = 4> **`drivemodel_folder_path`**: is the Google Drive's folder path where the trained classification neural network is going to be stored.\n",
        "\n",
        "drivemodel_folder_path = '' #@param {type:\"string\"}\n",
        "drivemodel_folder_path = drivemodel_folder_path + \"/\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "URQkaKN6S4Z9"
      },
      "source": [
        "#@markdown #Please specify the main training parameters.\n",
        "\n",
        "#@markdown ### Please select whether you want to use the default classification parameters.\n",
        "\n",
        "Use_default_classification_parameters = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### Otherwise, please define the parameters:\n",
        "\n",
        "#@markdown <font size = 4>**`initial_epochs`**: number of epochs for the first training stage. If `Use_default_classification_parameters` is enabled, the parameter is set to 10.\n",
        "\n",
        "initial_epochs = 10 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown <font size = 4>**`fine_tune_epochs`**: number of epochs for the second training stage. If `Use_default_classification_parameters` is enabled, the parameter is set to 10.\n",
        "\n",
        "fine_tune_epochs = 10 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown <font size = 4>**`batch_size`**: This parameter describes the amount of images that are loaded into the network per step. Smaller batchsizes may improve training performance slightly but may increase training time. If the notebook crashes while loading the dataset this can be due to a too large batch size. Decrease the number in this case. If `Use_default_classification_parameters` is enabled, the parameter is set to 4.\n",
        "\n",
        "batch_size =  2 #@param [2, 4, 8, 16, 32, 64, 128, 256] {type:\"raw\"}\n",
        "\n",
        "#@markdown <font size = 4>**`percentage_validation`**: percentage of your training dataset you want to use to validate the network during training. If `Use_default_classification_parameters` is enabled, the parameter is set to 10.\n",
        "\n",
        "percentage_validation = 10  #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "\n",
        "#@markdown <font size = 4>**`EarlyStopping_patience`**: number of epochs with no improvement after which training will be stopped when using the EarlyStopping method. If `Use_default_classification_parameters` is enabled, the parameter is set to 3.\n",
        "\n",
        "patience = 3 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "#@markdown <font size = 4>**`monitor_callback`**: metric to be monitored on the EarlyStopping and ModelCheckpoint methods. If `Use_default_classification_parameters` is enabled, the parameter is set to \"val_loss\".\n",
        "\n",
        "monitor_callback = \"val_loss\" #@param [\"accuracy\", \"loss\", \"val_accuracy\", \"val_loss\"]\n",
        "\n",
        "#@markdown ###Are you debugging?\n",
        "\n",
        "#@markdown <font size = 4>**`DEBUG`**: enable the parameter if you want to know some extra information for each step of the classification block\n",
        "\n",
        "DEBUG = True #@param {type:\"boolean\"}\n",
        "\n",
        "if Use_default_classification_parameters: \n",
        "  print(\"Default classification parameters enabled\")\n",
        "  initial_epochs = 10 \n",
        "  fine_tune_epochs = 10\n",
        "  batch_size = 4\n",
        "  percentage_validation = 10\n",
        "  patience = 3\n",
        "  monitor_callback = \"val_loss\"\n",
        "\n",
        "img_height = 299\n",
        "img_width = 299\n",
        "img_size = (img_height, img_width)\n",
        "total_epochs =  initial_epochs + fine_tune_epochs\n",
        "\n",
        "class_weights = {}\n",
        "for idx, item in enumerate(sorted(cell_lines_list)):\n",
        "  class_weights[idx] = max(number_of_snippets) / number_of_snippets[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsVSsDFMPvJ6"
      },
      "source": [
        "## **7.2. Creation of train and validation dataset**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxW3HN5mxNFh",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run to create the training  and validation datasets.\n",
        "\n",
        "print(\"TRAINING DATASET\") \n",
        "train_dataset = image_dataset_from_directory(\n",
        "    directory = tlsnippets_folder_path,\n",
        "    labels = \"inferred\",\n",
        "    label_mode = \"int\",\n",
        "    class_names = None, \n",
        "    color_mode = \"rgb\", \n",
        "    batch_size = batch_size, \n",
        "    image_size = (img_height, img_width),\n",
        "    shuffle = True, \n",
        "    seed = 32, \n",
        "    validation_split = (percentage_validation / 100), \n",
        "    subset = \"training\",\n",
        "    follow_links = False\n",
        ")\n",
        "print(f\"Number of training batches: {tf.data.experimental.cardinality(train_dataset)}\")\n",
        "\n",
        "print(\"\\n\") \n",
        "print(\"VALIDATION DATASET\") \n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    directory = tlsnippets_folder_path,\n",
        "    labels = \"inferred\",\n",
        "    label_mode = \"int\",\n",
        "    class_names = None, \n",
        "    color_mode = \"rgb\", \n",
        "    batch_size = batch_size, \n",
        "    image_size = (img_height, img_width),\n",
        "    shuffle = True, \n",
        "    seed = 32, \n",
        "    validation_split = (percentage_validation / 100), \n",
        "    subset = \"validation\", \n",
        "    follow_links = False\n",
        ")\n",
        "\n",
        "print(f\"Number of validation batches: {cardinality(validation_dataset)}.\")\n",
        "\n",
        "if DEBUG: \n",
        "  print(\"\\n\") \n",
        "  class_names = train_dataset.class_names\n",
        "  print(f\"Name of the classes found: {class_names}\")\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "if DEBUG: \n",
        "  print(\"\\n\") \n",
        "  print(\"Enabled buffered prefetching so you can yield data from disk without having I/O become blocking.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9r6zR4TSydQ"
      },
      "source": [
        "## **7.3. Set the data augmentation**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDsaz1LDS3b4",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run to configure the data augmentation layer, as well as display and save an example.\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  preprocessing.RandomFlip(mode = \"horizontal_and_vertical\"),\n",
        "  preprocessing.RandomRotation(0.2, fill_mode = 'reflect', interpolation = 'bilinear'),\n",
        "  preprocessing.RandomZoom(0.2, fill_mode = 'reflect', interpolation = 'bilinear')\n",
        "])\n",
        "\n",
        "for image, _ in train_dataset.take(1):\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  first_image = image[0]\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
        "    plt.imshow(augmented_image[0] / 255)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.savefig(os.path.join(model_folder_path, \"DataAugmentationExample.png\"), \n",
        "                dpi = 300, format = \"png\", bbox_inches = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuNTZvdyTP5C"
      },
      "source": [
        "## **7.4. Build the model**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NJyEYNaTUxt",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run to build the base model and stack the classification layers on top.\n",
        "\n",
        "# Preprocessing layer according to the model chosen\n",
        "preprocess_input = xception.preprocess_input\n",
        "\n",
        "# LOAD IN THE PRETRAINED BASE MODEL (AND PRETRAINED WEIGHTS)\n",
        "print(\"Loading in the pretrained base model (and pretrained weights)...\")\n",
        "print(\"\\n\")\n",
        "# Creation of the base model\n",
        "img_shape = img_size + (3,)\n",
        "base_model = xception.Xception(\n",
        "    include_top = False, \n",
        "    weights = 'imagenet',  \n",
        "    input_shape = img_shape\n",
        ")\n",
        "if DEBUG:\n",
        "  image_batch, label_batch = next(iter(train_dataset))\n",
        "  feature_batch = base_model(image_batch)\n",
        "  print(\"The base model act as a feature extractor.\")\n",
        "  print(f\"Snippets are reshaped from 299x299x3 into a {feature_batch.shape[1:]} block of features.\")\n",
        "  print(\"\\n\") \n",
        "\n",
        "# Freeze the convolutional base\n",
        "base_model.trainable = False\n",
        "if DEBUG:\n",
        "  print(\"Freezed the convolutional base.\")\n",
        "  print(\"It will prevent the weights of the base model from being updated during training.\")\n",
        "  base_model.summary()\n",
        "  print(\"\\n\") \n",
        "\n",
        "# STACK THE CLASSIFICATION LAYERS ON TOP\n",
        "print(\"Stacking the classification layers on top...\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Addition of a classification head\n",
        "global_average_layer = GlobalAveragePooling2D()\n",
        "if DEBUG:\n",
        "  feature_batch_average = global_average_layer(feature_batch)\n",
        "  print(\"Applied a GlobalAveragePooling2D() layer.\")\n",
        "  print(f\"To generate predictions from the block of features, average over the spatial {feature_batch.shape[1:3]} spatial locations.\")\n",
        "  print(f\"In order to convert the features to a single {feature_batch_average.shape[1]}-element vector per image.\")\n",
        "  print(\"\\n\") \n",
        "\n",
        "num_classes = len(class_names)\n",
        "prediction_layer = Dense(num_classes)\n",
        "if DEBUG:\n",
        "  prediction_batch = prediction_layer(feature_batch_average)\n",
        "  print(\"Applied a Dense() layer.\")\n",
        "  print(\"Convert features into predictions according to the number of classes.\")\n",
        "  print(f\"There is no need to apply activation function because prediction will be treated as a logit of {prediction_batch.shape[1]} outputs.\")\n",
        "  print(\"\\n\") \n",
        "  \n",
        "# Build the model\n",
        "inputs = keras.Input(shape = img_shape)\n",
        "x = data_augmentation(inputs)\n",
        "x = preprocess_input(x)\n",
        "x = base_model(x, training = False)\n",
        "x = global_average_layer(x)\n",
        "x = Dropout(0.2)(x)\n",
        "outputs = prediction_layer(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "print(\"The model has been composed correctly!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uArUILYY83UZ"
      },
      "source": [
        "## **7.5. Train the model**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUYbu8uZOZkQ"
      },
      "source": [
        "### **7.5.1. Train the classification layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPU6c8sEUysU",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run to compile the model.\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "model.compile(\n",
        "    optimizer = Nadam(learning_rate = base_learning_rate, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-07, name='Nadam'),\n",
        "    loss = SparseCategoricalCrossentropy(from_logits = True), \n",
        "    metrics = [\"accuracy\"]\n",
        ")\n",
        "\n",
        "if DEBUG:\n",
        "  model.summary()\n",
        "  print(\"\\n\") \n",
        "  print(\"Base model being used as a fixed feature structure.\")\n",
        "  print(\"Just the classification head layers being trained and updated.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJif7kvDWCgY",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run to train the top classification layers of the model.\n",
        "\n",
        "checkpoint_cb = ModelCheckpoint(\n",
        "    os.path.join(drivemodel_folder_path, \"weights_best.hdf5\"), \n",
        "    monitor = monitor_callback, \n",
        "    verbose = 1, \n",
        "    save_best_only = True,\n",
        "    mode = \"auto\",\n",
        "    save_weights_only = False,\n",
        "    save_freq = 'epoch',\n",
        ")\n",
        "\n",
        "csvlogger_cb = CSVLogger(\n",
        "    os.path.join(model_folder_path, \"model_log.csv\"),\n",
        "    separator = ',', \n",
        "    append = False\n",
        ")\n",
        "\n",
        "if DEBUG:\n",
        "  print(\"Set ModelCheckpoint() as a callback for the model.\")\n",
        "  print(f\"ModelCheckpoint() is monitoring {monitor_callback} during training of the model.\")\n",
        "  print(\"\\n\")\n",
        "  print(\"Set CSVLogger() as a callback for the model.\") \n",
        "  print(\"\\n\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs = initial_epochs,\n",
        "    callbacks = [checkpoint_cb, csvlogger_cb],\n",
        "    validation_data = validation_dataset,\n",
        "    class_weight = class_weights,\n",
        "    verbose = 1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7soplbChdOgt",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run to display and save the learning curves of the training of the top classification layers of the model.\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "initial_epoch = []\n",
        "for e in history.epoch:\n",
        "  initial_epoch.append(e+1)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(initial_epoch, acc, label='Training Accuracy')\n",
        "plt.plot(initial_epoch, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.autoscale(enable=True, axis='both')\n",
        "plt.xticks(ticks = initial_epoch)\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(initial_epoch, loss, label='Training Loss')\n",
        "plt.plot(initial_epoch, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.ylabel('Sparse Categorical Crossentropy')\n",
        "plt.xlabel('epoch')\n",
        "plt.autoscale(enable=True, axis='both')\n",
        "plt.xticks(ticks = initial_epoch)\n",
        "\n",
        "plt.savefig(os.path.join(model_folder_path, \"LearningCurvesTopClassificationLayers.png\"),\n",
        "            dpi = 300, format = \"png\", bbox_inches = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCyNXJCyXyIH"
      },
      "source": [
        "### **7.5.2. Round of fine-tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPAMUB3tbmHZ",
        "cellView": "form"
      },
      "source": [
        "#@markdown ## Run to compile the model.\n",
        "\n",
        "# Unfreeze the convolutional base and set the bottom layers to be un-trainable\n",
        "base_model.trainable = True\n",
        "fine_tune_at = round(len(base_model.layers) * (2/3))\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False\n",
        "if DEBUG:\n",
        "  print(f\"There are {len(base_model.layers)} layers on the base model\")\n",
        "  print(f\"Fine-tuning from {fine_tune_at} layer onwards.\")\n",
        "  print(\"\\n\") \n",
        "\n",
        "model.compile(\n",
        "    optimizer = Nadam(learning_rate = base_learning_rate / 10, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-07, name = 'Nadam'),\n",
        "    loss = SparseCategoricalCrossentropy(from_logits = True),\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "if DEBUG:\n",
        "  print(\"Model compiled!\")\n",
        "  model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuLXZ_ODVv_W",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run to train the entire model.\n",
        "\n",
        "# Set EarlyStopping callback\n",
        "earlystopping_cb = EarlyStopping(\n",
        "    monitor = monitor_callback, \n",
        "    min_delta = 0, \n",
        "    patience = patience, \n",
        "    verbose = 1,\n",
        "    mode = 'auto', \n",
        ")\n",
        "\n",
        "csvlogger_cb.append = True\n",
        "\n",
        "if DEBUG:\n",
        "  print(\"Set ModelCheckpoint() as a callback for the model.\")\n",
        "  print(f\"ModelCheckpoint() is monitoring {monitor_callback} during training of the model.\")\n",
        "  print(\"\\n\")\n",
        "if DEBUG:\n",
        "  print(\"Set EarlyStopping() as a callback for the model.\")\n",
        "  print(f\"EarlyStopping()'s patience = {patience}.\")\n",
        "  print(\"\\n\") \n",
        " \n",
        "history_fine = model.fit(\n",
        "    train_dataset,\n",
        "    epochs = total_epochs,\n",
        "    initial_epoch = history.epoch[-1]+1,\n",
        "    callbacks = [checkpoint_cb, earlystopping_cb, csvlogger_cb],\n",
        "    validation_data = validation_dataset,\n",
        "    class_weight = class_weights,\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "model.save(os.path.join(drivemodel_folder_path, 'weights_last.hdf5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgHUaV8mbl2D",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run to display and save the learning curves of the training of the entire model.\n",
        "\n",
        "acc += history_fine.history['accuracy']\n",
        "val_acc += history_fine.history['val_accuracy']\n",
        "loss += history_fine.history['loss']\n",
        "val_loss += history_fine.history['val_loss']\n",
        "\n",
        "epochs = []\n",
        "for e in history_fine.epoch:\n",
        "  epochs.append(e+1)\n",
        "\n",
        "fine_tune_epochs = initial_epoch + epochs\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(fine_tune_epochs, acc, label='Training Accuracy')\n",
        "plt.plot(fine_tune_epochs, val_acc, label='Validation Accuracy')\n",
        "plt.plot([initial_epoch[-1], initial_epoch[-1]],\n",
        "          plt.ylim(), label='Start Fine Tuning')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.autoscale(enable=True, axis='both')\n",
        "plt.xticks(ticks = fine_tune_epochs)\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(fine_tune_epochs, loss, label='Training Loss')\n",
        "plt.plot(fine_tune_epochs, val_loss, label='Validation Loss')\n",
        "plt.plot([initial_epoch[-1], initial_epoch[-1]],\n",
        "         plt.ylim(), label='Start Fine Tuning')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.ylabel('Sparse Categorical Crossentropy')\n",
        "plt.xlabel('epoch')\n",
        "plt.autoscale(enable=True, axis='both')\n",
        "plt.xticks(ticks = fine_tune_epochs)\n",
        "\n",
        "plt.savefig(os.path.join(model_folder_path, \"LearningCurvesFineTuning.png\"),\n",
        "            dpi = 300, format = \"png\", bbox_inches = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzhXWiY9bWpk"
      },
      "source": [
        "# **8. Download files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2x4l_g3dQOu",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Please select the folders to be downloaded.\n",
        "\n",
        "figures_folder = False #@param {type:\"boolean\"}\n",
        "model_folder = False #@param {type:\"boolean\"}\n",
        "regionprops_folder = False #@param {type:\"boolean\"}\n",
        "patches_folder = False #@param {type:\"boolean\"}\n",
        "snippets_folder = False #@param {type:\"boolean\"}\n",
        "\n",
        "if figures_folder:\n",
        "  !zip -r /content/WhoIsWho/Figures.zip /content/WhoIsWho/Figures\n",
        "\n",
        "if model_folder:\n",
        "  !zip -r /content/WhoIsWho/Model.zip /content/WhoIsWho/Model\n",
        "\n",
        "if regionprops_folder:\n",
        "  !zip -r /content/WhoIsWho/Regionpropsdf.zip /content/WhoIsWho/Patches/Regionpropsdf\n",
        "\n",
        "if patches_folder:\n",
        "  !zip -r /content/WhoIsWho/Patches.zip /content/WhoIsWho/Patches\n",
        "\n",
        "if snippets_folder:\n",
        "  !zip -r /content/WhoIsWho/Snippets.zip /content/WhoIsWho/Snippets\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj9jJG2h4FvC",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run to download as a zip the folders selected.\n",
        "\n",
        "if figures_folder:\n",
        "  files.download(\"/content/WhoIsWho/Figures.zip\")\n",
        "\n",
        "if model_folder:\n",
        "  files.download(\"/content/WhoIsWho/Model.zip\")\n",
        "\n",
        "if regionprops_folder:\n",
        "  files.download(\"/content/WhoIsWho/Regionpropsdf.zip\") \n",
        "\n",
        "if patches_folder:\n",
        "  files.download(\"/content/WhoIsWho/Patches.zip\")\n",
        "\n",
        "if snippets_folder:\n",
        "  files.download(\"/content/WhoIsWho/Snippets.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEy4RJecHm-O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}